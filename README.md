# Inspect AI Custom Scorers

This repository contains custom scorers designed to work with the [Inspect AI](https://github.com/UKGovernmentBEIS/inspect_ai/) framework, which is a Python repository for evaluating the performance of language models on various tasks.

## Overview

The scorers in this repository provide two different evaluation mechanisms:

1. **Fact Comparator Scorer**: This scorer evaluates the overlap between facts in a predefined target text and the actual output generated by a language model in response to a given input or question. It calculates two metrics: groundedness and thoroughness.

   The `FactComparator` class in `fact_comparator.py` is responsible for this evaluation. It uses a series of prompts to parse the input and target texts into lists of individual facts, and then compares these fact lists to determine the shared and unique facts. The groundedness metric measures the percentage of facts in the model's output that are present in the target text, while the thoroughness metric measures the percentage of facts in the target text that are present in the model's output. Each of these will be between 0 and 1.

2. **Prompt Scorer**: This scorer allows you to define a rubric in the target field, which is then used to evaluate the model's output. The rubric specifies whether the output should be considered a PASS or a FAIL based on certain criteria.

   The `PromptScorer` class in `prompt_scorer.py` implements this functionality. It takes the target text, which should contain instructions like "Return PASS if the answer contains that the sun is 4.6 billion years old, return FAIL otherwise," and the model's output. It then evaluates the output based on the provided criteria and returns a score of 1 (PASS) or 0 (FAIL).

## Installation

To use these scorers, you'll need to install the dependencies listed in the `requirements.txt` file.

## Testing

Very preliminary unit tests for the `FactComparator` and `PromptScorer` classes are provided in `test_fact_comparator.py` and `test_prompt_scorer.py`, respectively.