# Inspect AI Custom Scorers

This repository contains custom scorers designed to work with the [Inspect AI](https://github.com/UKGovernmentBEIS/inspect_ai/) framework, which is a Python repository for evaluating the performance of language models on various tasks.

## Overview

The scorers in this repository provide two different evaluation mechanisms:

1. **Fact Comparator Scorer**: This scorer evaluates the overlap between facts in a predefined target text and the actual output generated by a language model in response to a given input or question. It calculates two metrics: groundedness and thoroughness.
   The `FactComparator` class in [`inspect_ai_scorers.fact_comparator`](inspect_ai_scorers/fact_comparator.py) is responsible for this evaluation. It uses a series of prompts to parse the input and target texts into lists of individual facts, and then compares these fact lists to determine the shared and unique facts. The groundedness metric measures the percentage of facts in the model's output that are present in the target text, while the thoroughness metric measures the percentage of facts in the target text that are present in the model's output. Each of these will be between 0 and 1.

2. **Prompt Evaluator**: This scorer allows you to define a rubric in the target field, which is then used to evaluate the model's output. The rubric specifies whether the output should be considered a PASS or a FAIL based on certain criteria.
   The `PromptEvaluator` class in [`inspect_ai_scorers.prompt_evaluator`](inspect_ai_scorers/prompt_evaluator.py) implements this functionality. It takes the target text, which should contain instructions like "Return PASS if the answer contains that the sun is 4.6 billion years old, return FAIL otherwise," and the model's output. It then evaluates the output based on the provided criteria and returns a score of 1 (PASS) or 0 (FAIL).

## Testing

Preliminary unit tests for the `FactComparator` and `PromptEvaluator` classes are provided in the [`tests` directory](tests/). These don't test whether the scorers are doing a good job, they just test whether they can run in conjunction with a `Task` which queries a model, returns a response, and then evaluates that response with the scorer. Here, the `input` field is used for the question which is then being passed to a model to get the response.

## Examples

Examples of usage for the `FactComparator` and `PromptEvaluator` classes are provided in the [`examples` directory](examples/). Here, the `input` field is used for the text which is then evaluated by the scorer directly. These can be seen as tests of the scorers in conjunction with whatever evaluation models you use: for each example, there is a label for what the response should be, and if you run them, you'll see how well the model performs.

## License

This project is licensed under the [MIT License](LICENSE).

## Installation

To use these scorers, install the `inspect-ai-scorers` package from Test PyPI:

```
pip install --index-url https://test.pypi.org/simple/ inspect-ai-scorers
```